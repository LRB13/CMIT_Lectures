{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df3c5e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03c5afdd",
   "metadata": {},
   "source": [
    "# Linear Regression\n",
    "## d-dimensional case\n",
    "* Consider $\\mathbf{x} \\in \\mathbb{R}^d$ and $y \\in \\mathbb{R}$\n",
    "* Collect a set of $n$ points $\\{ (\\mathbf{x}_i , y_i ) \\}_{i=1}^n$\n",
    "* Aim: Find the hyperplane of best fit (equivalently line of best fit for $d=1$)\n",
    "* Consider a function $f(\\mathbf{x};\\theta)$ which maps $\\mathbf{x}_i$ onto $y_i$, parametrised by a set of parameters $\\theta$\n",
    "* We define the parameters $\\theta$ to contain two components: $\\theta = \\{ \\mathbf{w} , b \\}$, where $\\mathbf{w} \\in \\mathbb{R}^d$ and $b \\in \\mathbb{R}$, so that:\n",
    "$$  f(\\mathbf{x};\\theta) = \\mathbf{w}^T \\mathbf{x} + b$$\n",
    "\n",
    "## Solve via least squares\n",
    "### Closed Form Solution:\n",
    "* Given a set of points $\\{ (\\mathbf{x}_i , y_i ) \\}_{i=1}^n$, we can determine the parameters $\\theta$ via least squares optimisation:\n",
    "$$ \\min_{\\theta} \\mathcal{L}(\\theta) = \\min_{\\theta} \\frac{1}{2} \\sum_{i=1}^n \\big\\{ || f(\\mathbf{x};\\theta) - y_i ||_2^2 \\big\\} = \\min_{\\mathbf{w},b} \\frac{1}{2} \\sum_{i=1}^n \\big\\{ || \\mathbf{w}^T \\mathbf{x} + b - y_i ||_2^2 \\big\\} $$\n",
    "* Let $\\mathbf{\\hat{x}}_i = (1, \\mathbf{x}_i) \\in \\mathbb{R}^{d+1}$ and $W = (b,w_1,\\cdots,w_d) \\in \\mathbb{R}^{d+1}$, and let $\\mathbf{X} = (\\mathbf{\\hat{x_1}}, \\cdots, \\mathbf{\\hat{x_n}})^T \\in \\mathbb{R}^{n \\times d+1}$ and $\\mathbf{y} = (y_1, \\cdots, y_n)^T \\in \\mathbb{R}^n$, then the least squares problem has matrix form:\n",
    "$$ \\min_{\\theta} \\mathcal{L}(\\theta) =  \\min_{\\mathbf{W}} || \\mathbf{X} \\mathbf{W} - \\mathbf{y} ||_2^2 $$\n",
    "\n",
    "* This has a closed form solution, taken by setting the gradient to $0$:\n",
    "    * $\\nabla_{\\mathbf{W}} \\mathcal{L}(\\theta) = 2 \\mathbf{X}^T(\\mathbf{X} \\mathbf{W} - \\mathbf{y})$\n",
    "    * $ 2 \\mathbf{X}^T(\\mathbf{X} \\mathbf{W} - \\mathbf{y}) = 0 \\implies \\mathbf{W} = \\big( \\mathbf{X}^T \\mathbf{X} \\big)^{-1} \\mathbf{X}^T \\mathbf{y} $\n",
    "\n",
    "* In the following example we solve linear regression using numpy for a simple example where $d=1$ (i.e. $f(x;\\theta) = wx + b$, $w,b \\in \\mathbb{R}$)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb7afc0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solving the least-square problem in numpy\n",
    "# Fit a line, ``y = wx + b``, through some noisy data-points: \n",
    "x = np.array([0, 1, 2, 3])\n",
    "y = np.array([-1, 0.2, 0.9, 2.1])\n",
    "\n",
    "A = np.vstack([x, np.ones(len(x))]).T # vertical stack followed by a transposition. ( y = Ap, A=[[x 1]] and p =[[w],[b]] )\n",
    "print(\"The data matrix: \\n\", A) \n",
    "#numpy as a built in function achieving least squares\n",
    "w_np, b_np = np.linalg.lstsq(A, y, rcond=None)[0]\n",
    "print(\"The obtained result: \", w_np, b_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b07698d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the line of best fit\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(x, y, 'o', label='Original data', markersize=10)\n",
    "plt.plot(x, w_np*x + b_np, 'r', label='Fitted line')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "603f06ad",
   "metadata": {},
   "source": [
    "### Gradient Descent\n",
    "* It is often the case that a closed form solution is too computationally expensive. Gradient descent is a widely used alternative.\n",
    "* Start with some initial parameters $\\theta^{(0)}$ with some (small) time-step $\\tau$, gradient descent updates $\\theta^{(k)}$ iteratively by the following update:\n",
    "\n",
    "$$ \\theta^{(k+1)} = \\theta^{(k)} - \\tau \\nabla_{\\theta} \\mathcal{L}(\\theta^{(k)}$$\n",
    "\n",
    "For the following least squares problem ($\\mathbf{x},\\mathbf{w} \\in \\mathbb{R}^d, y \\in \\mathbb{R})$:\n",
    "$$ \\min_{\\theta} \\mathcal{L}(\\theta) = \\min_{\\theta} \\sum_{i=1}^n \\big\\{ || f(\\mathbf{x};\\theta) - y_i ||_2^2 \\big\\} = \\min_{\\mathbf{w},b} \\sum_{i=1}^n \\big\\{ || \\mathbf{w}^T \\mathbf{x} + b - y_i ||_2^2 \\big\\} $$\n",
    "We have the following gradients:\n",
    "$$ \\nabla_{\\mathbf{w}} \\mathcal{L}(\\theta) = \\sum_{i=1}^n \\mathbf{x}_i ( \\mathbf{w}^T \\mathbf{x}_i + b - y_i) $$\n",
    "$$ \\nabla_{b} \\mathcal{L}(\\theta) = \\sum_{i=1}^n ( \\mathbf{w}^T \\mathbf{x}_i + b - y_i) $$\n",
    "And so:\n",
    "$$ \\mathbf{w}^{(k+1)} = \\mathbf{w}^{(k)} - \\tau \\sum_{i=1}^n \\mathbf{x}_i ( \\mathbf{w^{(k)}}^T \\mathbf{x}_i + b^{(k)} - y_i) $$\n",
    "$$ b^{(k+1)} = b^{(k)} - \\tau \\sum_{i=1}^n  ( \\mathbf{w^{(k)}}^T \\mathbf{x}_i + b^{(k)} - y_i) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "300d453a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Solving least squares using gradient descent\n",
    "\n",
    "# Fit a line, ``y = wx + b``, through some noisy data-points, where d=1.\n",
    "x = np.array([0, 1, 2, 3])\n",
    "y = np.array([-1, 0.2, 0.9, 2.1])\n",
    "\n",
    "def fit_lse(x, y):\n",
    "    lr = 0.1 # the learning rate\n",
    "    w = 1\n",
    "    b = 0\n",
    "    for i in range(10):\n",
    "        # initializing the gradiant w.r.t w and b\n",
    "        grad_w = 0. \n",
    "        grad_b = 0. \n",
    "        \n",
    "        # Going through the entire dataset\n",
    "        for _x, _y in zip(x, y):\n",
    "            # accumulating the gradient w.r.t w and b\n",
    "            grad_w += _x*(_x*w + b - _y) \n",
    "            grad_b += (_x*w + b - _y)\n",
    "        \n",
    "        # Gradient descent steps\n",
    "        w = w - lr*(grad_w)\n",
    "        b = b - lr*(grad_b)\n",
    "        \n",
    "    return w, b\n",
    "\n",
    "w, b = fit_lse(x, y)\n",
    "print(\"The obtained result: \", w, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69baf0b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the line of best fit\n",
    "\n",
    "plt.figure(figsize=(12,4))\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(x, y, 'o', label='Original data', markersize=10)\n",
    "plt.plot(x, w_np*x + b_np, 'r', label='Fitted line')\n",
    "plt.title(\"Solved using numpy function\")\n",
    "plt.legend()\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(x, y, 'o', label='Original data', markersize=10)\n",
    "plt.plot(x, w*x + b, 'r', label='Fitted line')\n",
    "plt.title(\"Solved using gradient descent\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1672ce4",
   "metadata": {},
   "source": [
    "## Higher order polynomial regression\n",
    "* Linear functions sometimes don't capture the complexity of data, we can instead use higher order polynomials: $$f(\\mathbf{x}; \\theta) = b + w_1 \\mathbf{x} + w_2 \\mathbf{x}^2 + w_3 \\mathbf{x}^3 + \\cdots .$$\n",
    "* Consider another example using quadratics. For simplicity let $d=1$ so that $f(x,\\theta) = b + w_1 x + w_2 x^2$, $\\theta = \\{ b , w_1, w_2 \\}$. We have:\n",
    "$$ \\min_{\\theta} \\mathcal{L}(\\theta) = \\min_{b, w_1, w_2} \\sum_{i=1}^n || b + w_1 x_i + w_2 x_i^2 - y_i ||_2^2 $$\n",
    "So that:\n",
    "$$ \\nabla_b \\mathcal{L}(\\theta) = \\sum_{i=1}^n   ( b + w_1 x_i + w_2 x_i^2 - y_i) $$\n",
    "$$ \\nabla_{w_1} \\mathcal{L}(\\theta) = \\sum_{i=1}^n  x_i ( b + w_1 x_i + w_2 x_i^2 - y_i) $$\n",
    "$$ \\nabla_{w_2} \\mathcal{L}(\\theta) = \\sum_{i=1}^n  x_i^2 ( b + w_1 x_i + w_2 x_i^2 - y_i) $$\n",
    "\n",
    "$$ b^{(k+1)} = b^{(k)} - \\tau \\nabla_{b} \\mathcal{L}(b^{(k)}$$\n",
    "$$ w_1^{(k+1)} = w_1^{(k)} - \\tau \\nabla_{w_1} \\mathcal{L}(w_1^{(k)}$$\n",
    "$$ w_2^{(k+1)} = w_2^{(k)} - \\tau \\nabla_{w_2} \\mathcal{L}(w_2^{(k)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccb8c61e",
   "metadata": {},
   "source": [
    "## Homework:\n",
    "Complete the following function to do gradient descent applied to quadratic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80184314",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Homework:\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x = np.array([0.0, 1.0, 2.0, 3.0,  4.0,  5.0])\n",
    "y = np.array([0.0, 0.8, 0.9, 0.1, -0.8, -1.0])\n",
    "plt.plot(x, y, 'o', label='Original data', markersize=10)\n",
    "\n",
    "def fit_lse_quadratic(x, y):\n",
    "    lr = 0.001 # the learning rate, 'tau'\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "        \n",
    "    return b, w1, w2\n",
    "\n",
    "b, w1, w2 = fit_lse_quadratic(x, y)\n",
    "print(\"The obtained result: \", b, w1, w2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe03f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting \n",
    "plt.plot(x, y, 'o', label='Original data', markersize=10)\n",
    "plt.plot(x, w2*np.square(x)+ w1*x + b, 'r', label='Fitted line')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6db47dbf",
   "metadata": {},
   "source": [
    "# Binary Logistic Regression\n",
    "\n",
    "* Suppose we have a set of points $\\big\\{ ( \\mathbf{x}_i \\in \\mathbb{R}^d, y_i \\in \\{ 0, 1 \\} ) \\big\\}_{i=1}^n$\n",
    "* The aim is to build a classification model. We define a function $f$ parameterised by parameters $\\theta = \\{\\mathbf{w}, b\\}$, similar to linear regression, but this time based on a sigmoid function.\n",
    "$$ f(\\mathbf{x}) = \\frac{1}{1 + \\exp(-( \\mathbf{x}^T \\mathbf{w} + b))} $$\n",
    "Then is $y$ is found by thresholding at $\\frac{1}{2}$:\n",
    "$$ y = \\begin{cases}\n",
    "1, \\text{ if } f (\\mathbf{x} ; \\theta ) \\geq \\frac{1}{2} \\\\\n",
    "0, \\text{ else}\n",
    "\\end{cases} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67db4c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Illustration of the sigmoid function\n",
    "x = np.linspace(-10,10,100)\n",
    "sigmoid = 1./(1. + np.exp(-x))\n",
    "\n",
    "\n",
    "plt.plot(x, sigmoid)\n",
    "plt.legend([\"The Sigmoid function\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4c23e77",
   "metadata": {},
   "source": [
    "### Log-Likelihood as loss function\n",
    "* We use a probabilistic interpreation of the sigmoid function (Bernoulli distribution):\n",
    "$$ p(y=1 | \\mathbf{x} ; \\theta ) = f(\\mathbf{x};\\theta),$$\n",
    "$$ p(y=0 | \\mathbf{x} ; \\theta ) = 1- f(\\mathbf{x};\\theta),$$\n",
    "* The likelihood, $L(\\theta)$ is defined as:\n",
    "\\begin{align} L(\\theta) = p(y|\\mathbf{x};\\theta) &= f(\\mathbf{x};\\theta)^y ( 1 - f(\\mathbf{x};\\theta))^y \\\\\n",
    "&= \\prod_{i=1}^n f(\\mathbf{x}_i;\\theta)^{y_i} ( 1 - f(\\mathbf{x}_i;\\theta))^{y_i} \\end{align}\n",
    "* We now define the log-likelihood as:\n",
    "\\begin{align} \\log L(\\theta) &=  \\sum_{i=1}^n y_i \\log f(\\mathbf{x_i} ; \\theta) + (1-y_i) \\log ( 1 - f(\\mathbf{x_i};\\theta)) \\end{align}\n",
    "\n",
    "* We now convert this log-likelihood into a convex function that we can, then, minimize to update our parameters.\n",
    "\n",
    "* The standard logistic regression loss function is given by the negative log-likelihood (a.k.a the cross-entropy loss): \n",
    "$$ \\mathcal{L}(\\theta) = -\\log L(\\theta) $$\n",
    "\n",
    "* Next we perform gradient descent to update the parameters. The gradients are as follows:\n",
    "\\begin{align}\n",
    "\\nabla_{\\mathbf{w}} \\mathcal{L}(\\theta) &= - \\sum_{i=1}^n y_i \\nabla_{\\mathbf{w}} \\log f(\\mathbf{x_i}; \\theta) + (1-y_i) \\nabla_{\\mathbf{w}} \\log (1-f(\\mathbf{x_i}; \\theta) ), \\\\\n",
    "&= - \\sum_{i=1}^n \\Big( y_i \\frac{1}{f(\\mathbf{x_i}; \\theta)} + (1-y_i) \\frac{-1}{1 - f(\\mathbf{x_i}; \\theta)} \\Big)    \\nabla_{\\mathbf{w}} f(\\mathbf{x_i}; \\theta), \\\\\n",
    "&= - \\sum_{i=1}^n \\Big( y_i \\frac{1}{f(\\mathbf{x_i}; \\theta)} + (1-y_i) \\frac{-1}{1 - f(\\mathbf{x_i}; \\theta)} \\Big)    f(\\mathbf{x_i}; \\theta) (1 - f(\\mathbf{x_i}; \\theta)) \\nabla_{\\mathbf{w}}(\\mathbf{x_i}^T \\mathbf{w} + b), \\\\\n",
    "&= - \\sum_{i=1}^n \\Big( y_i \\frac{1}{f(\\mathbf{x_i}; \\theta)} + (1-y_i) \\frac{-1}{1 - f(\\mathbf{x_i}; \\theta)} \\Big)    f(\\mathbf{x_i}; \\theta) (1 - f(\\mathbf{x_i}; \\theta)) \\mathbf{x_i}, \\\\\n",
    "& = - \\sum_{i=1}^n \\big( y_i - f(\\mathbf{x_i};\\theta) \\big) \\mathbf{x_i}\n",
    "\\end{align}\n",
    "\n",
    "\\begin{align}\n",
    "\\nabla_{b} \\mathcal{L}(\\theta)  &= - \\sum_{i=1}^n \\Big( y_i \\frac{1}{f(\\mathbf{x_i}; \\theta)} + (1-y_i) \\frac{-1}{1 - f(\\mathbf{x_i}; \\theta)} \\Big)    f(\\mathbf{x_i}; \\theta) (1 - f(\\mathbf{x_i}; \\theta)) \\nabla_{b}(\\mathbf{x_i}^T \\mathbf{w} + b), \\\\\n",
    "&= - \\sum_{i=1}^n \\Big( y_i \\frac{1}{f(\\mathbf{x_i}; \\theta)} + (1-y_i) \\frac{-1}{1 - f(\\mathbf{x_i}; \\theta)} \\Big)    f(\\mathbf{x_i}; \\theta) (1 - f(\\mathbf{x_i}; \\theta)) , \\\\\n",
    "& = - \\sum_{i=1}^n \\big( y_i - f(\\mathbf{x_i};\\theta) \\big)\n",
    "\\end{align}\n",
    "\n",
    "## Implementing a logistic regressor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eea7664",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class MyBinaryLogisticRegression:\n",
    "    def __init__(self, n_iter, lr):\n",
    "        \"\"\"\n",
    "        A class for binary logistic regression model\n",
    "         \n",
    "        n_iter: (integer)\n",
    "            Number of training iteration\n",
    "        lr: (float)\n",
    "            learning rate\n",
    "        \"\"\"\n",
    "\n",
    "        self.n_iter = n_iter\n",
    "        self.lr = lr\n",
    "        \n",
    "        self.losses = []\n",
    "        self.grads = []\n",
    "\n",
    "    def init_params(self, n_feats):\n",
    "        self.w = np.zeros((1, n_feats))\n",
    "        self.b = 0.\n",
    "        \n",
    "    def _optimize(self, X, y):\n",
    "        m = X.shape[0]\n",
    "\n",
    "        # cost function\n",
    "        probs = self.activation(X) \n",
    "        cost = (-1/m)*(np.sum((y*np.log(probs)) + ((1-y)*(np.log(1-probs)))))\n",
    "\n",
    "        # computing the gradient \n",
    "        dw = (1/m)*(np.dot((probs - y), X)) \n",
    "        \n",
    "\n",
    "            \n",
    "        db = (1/m)*(np.sum(probs - y))\n",
    "\n",
    "        grads = {\"dLdw\": dw, \"dLdb\": db}\n",
    "\n",
    "        return grads, cost\n",
    "\n",
    "    def activation(self, X):  \n",
    "        \"\"\"\n",
    "            Compute the Sigmoid activation on the dataset\n",
    "            \n",
    "            X: (np-array) shape=(n_samples x n_feats)\n",
    "                data matrix \n",
    "        \"\"\"          \n",
    "        return 1./(1. + np.exp(-np.dot(self.w, X.T) - self.b))\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "            Training the model\n",
    "            \n",
    "            X: (np-array) shape=(n_samples x n_feats)\n",
    "                data matrix\n",
    "            y: (np-array) shape=( n_samples)\n",
    "                targets \n",
    "        \"\"\"\n",
    "        \n",
    "        self.init_params(X.shape[1])\n",
    "        \n",
    "        for i in range(self.n_iter):\n",
    "            grads, cost = self._optimize(X, y)\n",
    "            #\n",
    "            dLdw = grads['dLdw']\n",
    "            dLdb = grads['dLdb']\n",
    "            \n",
    "            # gradient descent\n",
    "            self.w = self.w - self.lr * dLdw\n",
    "            self.b = self.b - self.lr * dLdb\n",
    "            \n",
    "            self.losses.append(cost)\n",
    "            self.grads.append(grads)\n",
    "            #if (i % 10 == 0):\n",
    "            #    print(\"Standard Logistic Regression: Iter {}, Cost {}\".format(i, cost))\n",
    "            \n",
    "        print(\"Standard Logistic Regression: Iter {}, Cost {}\".format(i, cost))\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "            Predicting the discrete labels\n",
    "            \n",
    "            X: (np-array) shape=(n_samples x n_feats)\n",
    "                data matrix \n",
    "        \"\"\"\n",
    "        activ = self.activation(X)\n",
    "        return activ >= 0.5 \n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        \n",
    "        \"\"\"\n",
    "            Predicting the probabilities\n",
    "\n",
    "            X: (np-array) shape=(n_samples x n_feats)\n",
    "                data matrix \n",
    "        \"\"\"\n",
    "        return self.activation(X) \n",
    "    \n",
    "    def score(self, X, y): \n",
    "        \"\"\"\n",
    "            computing the accuracy of the model\n",
    "            \n",
    "            X: (np-array) shape=(n_samples x n_feats)\n",
    "                data matrix \n",
    "        \"\"\"\n",
    "        pred = self.predict(X)\n",
    "        return (pred == y).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25da8c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To Try out the model, we start by generating a random binary dataset\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "# we create 200 separable points\n",
    "X, y = make_classification(n_samples=200, n_features=2, n_redundant=0, n_informative=2,\n",
    "                           random_state=0, n_clusters_per_class=2, class_sep=2.0)\n",
    "plt.figure()\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, edgecolors='black', s=25) \n",
    "plt.title('Plot of the generated data and their label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0d43757",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instanciating the model and training\n",
    "my_clf = MyBinaryLogisticRegression(n_iter=100, lr=0.1)\n",
    "my_clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b439eefb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the decision boundary of our Binary Logistic Regressor\n",
    "def print_decision(X, y, clf, title=\"Decision Boundary\"):\n",
    "    plt.figure()\n",
    "    h = .02\n",
    "    x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\n",
    "    y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                         np.arange(y_min, y_max, h))\n",
    "\n",
    "    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "\n",
    "    ax = plt.subplot(111)\n",
    "    ax.contourf(xx, yy, Z, alpha=.8)\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, edgecolors='black', s=25) \n",
    "    plt.title(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e478a5d3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print_decision(X, y,  my_clf, \"Decision Boundary of our Binary Logistic Regressor\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37bc8ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "cost = my_clf.losses\n",
    "plt.plot(cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a192bb07",
   "metadata": {},
   "source": [
    "## Recommended resources\n",
    "- [3Blue1Brown's Neural Network Series](https://www.youtube.com/watch?v=aircAruvnKk&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86f2ef31",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0115b123",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
