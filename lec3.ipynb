{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "350546b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "#install torchvision via anaconda prompt:\n",
    "#  conda install -c pytorch torchvision "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b8416a8",
   "metadata": {},
   "source": [
    "## 1. Definition\n",
    "\n",
    "A feed-forward neural network $f(\\cdot \\theta)$ is a **directed acyclic graph**, parametrized by $\\theta$, that applies a series of transformation to an input $\\mathbf{x} \\in \\mathbb{R}^d$, layer-wise, and without recursion, to\n",
    "produce an output $y = f(\\mathbf{x}, \\theta) \\in \\mathbb{R}^s$ , as depicted in below.\n",
    "<p align=\"center\\\">\n",
    "<img src=\"layer_wise_fnn.png\\\" width=\"550\" title=\"Layer computation in a feed-forward neural network\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0959807b",
   "metadata": {},
   "source": [
    "Given a $K$ layer network -- here the input is not considered as a layer --, the $k$-th layer is characterized by a function\n",
    "$f_k$ parametrized by $\\theta_k$. \n",
    "In other words, \n",
    "$$f(\\mathbf{x}, \\theta) = f_K(\\dots f_2(f_1(\\mathbf{x}; \\theta_1); \\theta_2) \\dots; \\theta_K).$$\n",
    "The $f_k$'s are of the form\n",
    "$$f_k(\\mathbf{z}_{k-1}; \\theta_k) = \\mathbf{z}_k = \\sigma_k(\\mathbf{q}_{k-1}) = \\sigma_k(\\mathbf{W}_k\\mathbf{z}_{k-1} + \\mathbf{b}_k),$$\n",
    "where $\\theta_k = \\{\\mathbf{W}_k, \\mathbf{b}_k\\}$, $\\mathbf{W}_k \\in \\mathbb{R}^{h_k\\times h_{k-1}}$ is known as the **weight\n",
    "matrix**, $\\mathbf{b}_k \\in \\mathbb{R}^{h_k}$ is the **bias vector**, $\\mathbf{z}_k \\in \\mathbb{R}^{h_k}$ is the output\n",
    "of the $k$-th layer, $h_k$ is the dimension (the number of neurons) of the $k$-th layer, $\\sigma_k$ is point-wise operator \n",
    "known as **activation function** of the layer, and $\\mathbf{q}_k$ is the **pre-activation vector**.\n",
    "\n",
    "\n",
    "Typical **activation functions** include: \n",
    "- **tanh**: $x \\mapsto \\frac{\\exp(2x) -1 }{\\exp(2x) + 1}$,\n",
    "- **sigmoid**: $x \\mapsto \\frac{1}{\\exp(-x) + 1}$,\n",
    "- **ReLU**:  $x \\mapsto \\max(0,x)$ (The Rectified Linear Unit ),\n",
    "- **softmax**:  $\\mathbf{x} \\in \\mathbb{R}^d  \\mapsto [\\dots, \\frac{\\exp(\\mathbf{x}[i])}{\\sum_{j=1}^d \\exp(\\mathbf{x}[j])}, \\dots]$.\n",
    "\n",
    "\n",
    "Given an output $\\mathbf{z} \\in \\mathbb{R}^s$ and a ground-truth $\\mathbf{y} \\in \\mathbb{R}^s$, commonly used **loss functions** are:\n",
    "- **MSE**: $(\\mathbf{z}, \\mathbf{y}) \\mapsto \\|\\mathbf{z} - \\mathbf{y}\\|_2^2$, (Mean Squared Error),\n",
    "- **MAE**: $(\\mathbf{z}, \\mathbf{y}) \\mapsto \\|\\mathbf{z} - \\mathbf{y}\\|_1$, (Mean Absolute Error),\n",
    "- **CE**: $(\\mathbf{z}, \\mathbf{y}) \\mapsto - \\langle \\mathbf{y}, \\log\\mathbf{z} \\rangle = - \\sum_i \\mathbf{y}[i] \\log \\mathbf{z}[i] $, (Cross-entropy).\n",
    "\n",
    "    \n",
    "**Illustration of a FNN**\n",
    "<p align=\"center\\\">\n",
    "<img src=\"fnn_example.png\\\" width=\"450\" title=\"Graphical view of a FNN\">\n",
    "</p> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dab9fe0d",
   "metadata": {},
   "source": [
    "## 2. Back-propagation\n",
    "\n",
    "### 2.1 Basic idea\n",
    "\n",
    "Given a set of network parameter $\\theta$, and a dataset $\\mathcal{D} = \\{\\mathbf{x}_i \\in \\mathbb{R}^d, \\mathbf{y}_i \\in \\mathbb{R}^s\\}$,\n",
    "the goal is to optimize the following problem\n",
    "$$\n",
    "\\underset{\\theta}{ \\text{ minimize } } \\Big[ \\ell(\\theta) =  \\frac{1}{n} \\sum_{i=1}^n loss(f(\\mathbf{x}_i; \\theta), \\mathbf{y}_i) \\Big].\n",
    "$$\n",
    "\n",
    "Not only is the problem  in high dimension, $\\text{dim}(\\theta) > 10$, it is also non-convex. Therefore, $\\theta$ is updated iteratively based on first order information (the gradient) so as to reach a (local) minimum.\n",
    "\n",
    "Let $\\theta^t$ be the value of $\\theta$ at the $t$-th iteration, $\\theta^0$ being the initialization of the parameters of the network,\n",
    "the update rule of the standard gradient descent  is as follows,\n",
    "$$\n",
    "\\theta^{t+1} \\leftarrow \\theta^{t} - \\eta \\nabla_{\\theta^{t}}\\ell(\\theta^{t}) ,\n",
    "$$\n",
    "\n",
    "where $\\nabla_{\\theta} \\ell(\\theta)$ is the **gradient** of $\\ell$ w.r.t to $\\theta$, and $\\eta$ is the \n",
    "step-size also known as **learning rate**. The parameters in $\\theta$ are updated sequentially layer after another,\n",
    "starting with the output one, by back-propagating the loss value, as illustrated in the figure below.\n",
    "\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"layer_wise_fnn_bprop.png\" width=\"750\" title=\"Back-propagation\">\n",
    "\n",
    "    \n",
    "The **learning rate** is usually chosen experimentally based on the figure below. In practice, various different variants of gradient descent are used, and are built in functions into both pytorch and tensorflow. Some commonly used optimisers are:\n",
    "- Stochastic gradient descent (SGD)\n",
    "- Adadelta\n",
    "- Adam\n",
    "    \n",
    "    <p align=\"center\">\n",
    "<img src=\"learningrates.jpeg\" width=\"350\" title=\"Back-propagation\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94eb575a",
   "metadata": {},
   "source": [
    "## 3. Training\n",
    "\n",
    "### 3.1 Avoid over-fitting\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"fittings.jpg\" width=\"550\" title=\"Types of fittings during training.\" >\n",
    "</p> \n",
    "\n",
    "### Separate the data into three folds\n",
    "<p align=\"center\\\">\n",
    "<img src=\"datasep.png\" width=\"350\" title=\"Dataset separation.\" >\n",
    "</p>\n",
    "\n",
    "### 3.2 Assessing the model performance on the validation set\"\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"accuracies.jpeg\" width=\"350\" title=\"Dataset separation.\" >\n",
    "</p>\n",
    "\n",
    "### 3.3 Further readings\n",
    "      - [Deep learning book](http://deeplearningbook.org/)\n",
    "      - [Stanford's CS231n](http://cs231n.github.io/)\n",
    "      - [Washington University in St. Louis](https://github.com/jeffheaton/t81_558_deep_learning)\n",
    "      - [Deep learning paper](http://www.cs.toronto.edu/~hinton/absps/NatureDeepReview.pdf)\n",
    "      - [Tensorflow/Keras tutorial](https://www.tensorflow.org/guide/keras)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46a790e2",
   "metadata": {},
   "source": [
    "## 4. Example - MNIST Classification\n",
    "\n",
    "- We will have a dataset $ \\mathcal{D} = \\{ (x_i,y_i) \\}$ of $n$ images $x_i$ and $n$ associated labels $y_i$, with $y_i = \\{0, 1, \\cdots, 9\\}$, $i=1,\\cdots,n$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71895c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "train_data = datasets.MNIST(\n",
    "    root = 'data',\n",
    "    train = True,                         \n",
    "    transform = ToTensor(), \n",
    "    download = True)\n",
    "test_data = datasets.MNIST(\n",
    "    root = 'data', \n",
    "    train = False, \n",
    "    transform = ToTensor()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e452bc7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print train_data\n",
    "print(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98073699",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print test_data\n",
    "print(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e73b553",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_data.data.size())\n",
    "print(train_data.targets.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6047c8c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualisation\n",
    "# Plot one train data\n",
    "import matplotlib.pyplot as plt\n",
    "i=4000\n",
    "plt.imshow(train_data.data[i], cmap='gray')\n",
    "plt.title('%i' % train_data.targets[i])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bbc6876",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot multiple\n",
    "figure = plt.figure(figsize=(10, 8))\n",
    "cols, rows = 5, 5\n",
    "for i in range(1, cols * rows + 1):\n",
    "    sample_idx = torch.randint(len(train_data), size=(1,)).item()\n",
    "    img, label = train_data[sample_idx]\n",
    "    figure.add_subplot(rows, cols, i)\n",
    "    plt.title(label)\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(img.squeeze(), cmap=\"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a57eac3f",
   "metadata": {},
   "source": [
    "## Preparing data for training with DataLoaders\n",
    "\n",
    "The Dataset retrieves our dataset’s features and labels one sample at a time. While training a model, we typically want to pass samples in “minibatches”, reshuffle the data at every epoch to reduce model overfitting, and use Python’s multiprocessing to speed up data retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f899eec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "loaders = {\n",
    "    'train' : torch.utils.data.DataLoader(train_data, \n",
    "                                          batch_size=100, \n",
    "                                          shuffle=True, \n",
    "                                          num_workers=1),\n",
    "    \n",
    "    'test'  : torch.utils.data.DataLoader(test_data, \n",
    "                                          batch_size=100, \n",
    "                                          shuffle=True, \n",
    "                                          num_workers=1),\n",
    "}\n",
    "loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5766a252",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdfbe41e",
   "metadata": {},
   "source": [
    "## Define network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0723289",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        self.hidden1 = nn.Linear(28*28,512)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.hidden2 = nn.Linear(512,10)\n",
    "        self.softmax = nn.Softmax()\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)  # flatten the input (batch_size, 28* 28)\n",
    "        x = self.hidden1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.hidden2(x)\n",
    "        x = self.softmax(x)\n",
    "        return x\n",
    "\n",
    "model = Net()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6f7ee7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr = learning_rate)   \n",
    "optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be27c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = nn.NLLLoss()   \n",
    "loss_func"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2c339e8",
   "metadata": {},
   "source": [
    "## Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e256a467",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "num_epochs = 10\n",
    "\n",
    "def train(num_epochs, model, loaders):\n",
    "    \n",
    "    model.train()\n",
    "        \n",
    "    # Train the model\n",
    "    total_step = len(loaders['train'])\n",
    "        \n",
    "    for epoch in range(num_epochs):\n",
    "        for i, (images, labels) in enumerate(loaders['train']):\n",
    "            \n",
    "            # gives batch data, normalize x when iterate train_loader\n",
    "            b_x = Variable(images)   # batch x\n",
    "            b_y = Variable(labels)   # batch y\n",
    "            output = model(b_x)#[0]    \n",
    "            \n",
    "            #print(b_x.shape)\n",
    "            #print(output.shape)\n",
    "            #print(b_y.shape)\n",
    "            loss = loss_func(output, b_y)\n",
    "            \n",
    "            # clear gradients for this training step   \n",
    "            optimizer.zero_grad()           \n",
    "            \n",
    "            # backpropagation, compute gradients \n",
    "            loss.backward()                # apply gradients             \n",
    "            optimizer.step()                \n",
    "            \n",
    "            if (i+1) % 100 == 0:\n",
    "                print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
    "                       .format(epoch + 1, num_epochs, i + 1, total_step, loss.item()))               \n",
    "        \n",
    "    \n",
    "    \n",
    "train(num_epochs, model, loaders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f06ef285",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    # Test the model\n",
    "    model.eval()    \n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for images, labels in loaders['test']:\n",
    "            test_output = model(images)\n",
    "            pred_y = torch.max(test_output, 1)[1].data.squeeze()\n",
    "            accuracy = (pred_y == labels).sum().item() / float(labels.size(0))\n",
    "        print('Test Accuracy of the model on the 10000 test images: %.2f' % accuracy)\n",
    "    \n",
    "    pass\n",
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb54fcda",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot multiple\n",
    "figure = plt.figure(figsize=(15, 12))\n",
    "cols, rows = 5, 5\n",
    "for i in range(1, cols * rows + 1):\n",
    "    sample_idx = torch.randint(len(test_data), size=(1,)).item()\n",
    "    img, label = test_data[sample_idx]\n",
    "    output = model(img)\n",
    "    pred_y = torch.max(output,1)\n",
    "    pred_label = pred_y[1].numpy()\n",
    "    figure.add_subplot(rows, cols, i)\n",
    "    plt.title('gt = ' + str(label) + ', pred = ' + str(pred_label))\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(img.squeeze(), cmap=\"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86667f3a",
   "metadata": {},
   "source": [
    "## 5. Image Denoising with MNIST\n",
    "\n",
    "- Problem setup: Suppose we have an observed noisy image $x$, and we wish to recover a clean version of the image, $u$. We model using additive gaussian noise: $x = u + \\epsilon$ where $\\epsilon$ is the noise.\n",
    "- Let us set the data up, by extracting the MNIST images and adding gaussian noise to them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31e22ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "\n",
    "train_imgs = train_data.data\n",
    "print(train_imgs.shape)\n",
    "train_imgs = train_imgs.numpy()\n",
    "# store the clean images before adding noise\n",
    "train_imgs0 = train_imgs/255.0\n",
    "\n",
    "test_imgs = test_data.data\n",
    "test_imgs = test_imgs.numpy()\n",
    "test_imgs0 = test_imgs/255.0\n",
    "\n",
    "sigma = 70/255.0 # noise level\n",
    "train_imgs = (train_imgs/255.0 + np.random.normal(0,sigma,train_imgs.shape)).clip(0,1)\n",
    "test_imgs = (test_imgs/255.0 + np.random.normal(0,sigma,test_imgs.shape)).clip(0,1)\n",
    "\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(train_imgs0[0],cmap='gray')\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(train_imgs[0],cmap='gray')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d48b151",
   "metadata": {},
   "source": [
    "## Convolutional Neural Networks\n",
    "Convolutional neural networks are particular types of FNN -- introduced in [this paper](http://yann.lecun.com/exdb/publis/pdf/lecun-89e.pdf) by Yann LeCun-- that takes into account the structure/topology of their inputs in the processing.\n",
    "\n",
    "They tackle the following points that are ignored in standard FNNs.\n",
    "\n",
    "- **Dealing with very high-dimensional inputs.** For instance, given an RGB image of size $ 200 \\times 200 \\times 3$, the weights matrix between the input and the first hidden layer, of dimension $h_1$, is of size $120000 \\times h_1$. As a result, not only will the number of parameters explode, but the computation will also be time-consuming.\n",
    "- **Exploiting the input topology (spatial structure)**, $2D$ or $3D$ images. As shown in the dictionary learning section, images can be written as linear combination of patterns. Accordingly, instead of have a large weight matrix, it may be better to have small ones that look for certain patterns.\n",
    "- **Building invariance to certain variations**, e.g. translation, illumination, etc. In an image classification task, the output should be remain the same, to extent, after small transformation on the input.\n",
    "\n",
    "To do so, the CNNs incorporate the following techniques.\n",
    "\n",
    "- **Local connectivity**: It removes the cumbersome and time-consuming fully connection between two\n",
    " layers by local connection. That is, each layer is divided into \\\\textit{feature maps} and the weights\n",
    " are applied on local regions of the input called receptive fields of .\n",
    "- **Parameter sharing**: Going further, all neurons of a feature map are forced to share\n",
    " the same weights. Consequently, in a feature map, the neurons look for the same pattern but at \n",
    " different locations in the input. The $i$-th feature map of the $l$-layer can, therefore, be seen as the result of \n",
    " a **cross-correlation** operation between its weights $\\mathbf{W}_l^i$ and its input $\\mathbf{z}_l$. \n",
    "By language abuse, this operation is called convolution thus the name **convolutional neural networks**.\n",
    "- **Pooling/sub-sampling**: This point aims at reducing the dimension of a layer by aggregating\n",
    "its feature maps structurally. Also, the aggregation can be done so as to make the output invariant\n",
    "to small translation by taking the maximum in a sliding-window manner\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"conv_pool_illust.png\" width=\"450\" title=\"Fully Convolutional Neural Network Example.\" >\n",
    "</p>\n",
    "\n",
    "Strongly recommend [Stanford's CS231n CNN Tutorial](https://cs231n.github.io/convolutional-networks/) for reading.\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"fcn.png\" width=\"1000\" title=\"Fully Convolutional Neural Network Example.\" >\n",
    "</p>\n",
    "\n",
    "### Define network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e6e8a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "# Input to conv1 will be image of shape [batch_size,1,28,28] (height and width are 28 for this example)\n",
    "        self.conv1 = nn.Sequential(   \n",
    "            nn.Conv2d(in_channels=1,out_channels=10,kernel_size=(3,3),padding=1), #output of this conv is of shape [BS,10,28,28]\n",
    "            nn.ReLU(), \n",
    "            nn.MaxPool2d(kernel_size=(2,2)) #output of this is [BS,10,14,14]\n",
    "        )\n",
    "        self.conv2 = nn.Sequential( \n",
    "            nn.Conv2d(in_channels=10,out_channels=20,kernel_size=(3,3),padding=1), #output of this is [BS,20,14,14]\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=(2,2)) # output of this is [BS,20,7,7]\n",
    "        )\n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=20,out_channels=30,kernel_size=(3,3),padding=1), #Output of this [BS,30,7,7]\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=30,out_channels=30,kernel_size=(3,3),padding=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.conv4 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=30,out_channels=20,kernel_size=(3,3),padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Upsample(scale_factor=2))\n",
    "        \n",
    "        self.conv5 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=20,out_channels=10,kernel_size=(3,3),padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Upsample(scale_factor=2))\n",
    "        \n",
    "        \n",
    "        self.conv6 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=10,out_channels=1,kernel_size=(1,1)),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "            \n",
    "\n",
    "    def forward(self, x):\n",
    "          \n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.conv4(x)\n",
    "        x = self.conv5(x)\n",
    "        x = self.conv6(x)\n",
    "        return x\n",
    "\n",
    "model = CNN()\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1441907d",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr = learning_rate)   \n",
    "optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a208bae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_step = len(train_imgs)\n",
    "print(total_step)\n",
    "train_imgs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c715b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaders = {\n",
    "    'train' : torch.utils.data.DataLoader(train_imgs, \n",
    "                                          batch_size=100, \n",
    "                                          shuffle=True, \n",
    "                                          num_workers=1),\n",
    "    \n",
    "    'test'  : torch.utils.data.DataLoader(test_imgs, \n",
    "                                          batch_size=100, \n",
    "                                          shuffle=True, \n",
    "                                          num_workers=1),\n",
    "}\n",
    "loaders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c46ee838",
   "metadata": {},
   "source": [
    "## Loss Function:\n",
    "\n",
    "- In our denoising setup we assume no label. Our dataset consists of only observed noisy images, i.e. $\\mathcal{D} = \\{ \\mathbf{x}_i \\in \\mathbb{R}^{h \\times w} \\}_{k=1}^n$. This approach (not using labels) is known as **unsupervised learning**.\n",
    "\n",
    "- We will implement a classic denoising model, which is composed of two terms:\n",
    "$$ \\mathcal{L}(\\theta) = \\sum_{k=1}^n || \\nabla f(\\mathbf{x}_i;\\theta) ||_2 + \\frac{\\lambda}{2} || f(\\mathbf{x}_i; \\theta) - \\mathbf{x}_i ||_2^2 , $$\n",
    "- where $\\lambda>0$ is a parameter which we hand tune according to the strength of noise. If we have a large $\\lambda$, the second term is more dominiant and our network output will be matched more closely to the input (i.e. $f(\\mathbf{x}_i ; \\theta) \\approx \\mathbf{x}_i$). If $\\lambda$ is small, the first term will be more dominant and more smoothing will occur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98527722",
   "metadata": {},
   "outputs": [],
   "source": [
    "def up_shift(f):\n",
    "    g = torch.zeros_like(f)\n",
    "    g[:-1, :] = f[1:, :]\n",
    "    g[-1, :] = f[-1, :]\n",
    "    return g\n",
    "\n",
    "def down_shift(f):\n",
    "    g = torch.zeros_like(f)\n",
    "    g[1:, :] = f[:-1, :]\n",
    "    g[0, :] = f[0, :]\n",
    "    return g\n",
    "\n",
    "def left_shift(f):\n",
    "    g = torch.zeros_like(f)\n",
    "    g[:, :-1] = f[:, 1:]\n",
    "    g[:, -1] = f[:, -1]\n",
    "    return g\n",
    "\n",
    "def right_shift(f):\n",
    "    g = torch.zeros_like(f)\n",
    "    g[:, 1:] = f[:, :-1]\n",
    "    g[:, 0] = f[:, 0]\n",
    "    return g\n",
    "\n",
    "\n",
    "def grad(f):\n",
    "    f_x = (left_shift(f) - right_shift(f))/2\n",
    "    f_y = (down_shift(f) - up_shift(f))/2\n",
    "    \n",
    "    return torch.sqrt(f_x**2 + f_y**2 + 1e-7)\n",
    "\n",
    "class denoising_loss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    def forward(self, denoised, noisy, lambdaP):\n",
    "        \n",
    "        TV_term = grad(denoised)\n",
    "        \n",
    "        Fit_Term = (lambdaP/2)*(denoised-noisy)**2\n",
    "        \n",
    "        loss = TV_term + Fit_Term\n",
    "\n",
    "        return loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bf4e4f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "num_epochs = 10\n",
    "lambdaP = 5\n",
    "loss_func = denoising_loss()\n",
    "def train(num_epochs, model, loaders):\n",
    "    \n",
    "    model.train()\n",
    "        \n",
    "    # Train the model\n",
    "    total_step = len(loaders['train'])\n",
    "        \n",
    "    for epoch in range(num_epochs):\n",
    "        for i, images in enumerate(loaders['train']):\n",
    "            \n",
    "            # gives batch data, normalize x when iterate train_loader\n",
    "            b_x = Variable(images)   # batch x. b_x is of shape [100,28,28]\n",
    "            #print(b_x.shape)\n",
    "            b_x = b_x.unsqueeze(1) # make dimensions [100,1,28,28], rather than [100,28,28]\n",
    "            #print(b_x.shape)\n",
    "            output = model(b_x.float())  \n",
    "\n",
    "            #print(output.shape)\n",
    "            loss = loss_func(output,b_x,lambdaP)\n",
    "                        \n",
    "            # clear gradients for this training step   \n",
    "            optimizer.zero_grad()           \n",
    "            \n",
    "            # backpropagation, compute gradients \n",
    "            loss.backward()                # apply gradients             \n",
    "            optimizer.step()                \n",
    "            \n",
    "            if (i+1) % 100 == 0:\n",
    "                print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
    "                       .format(epoch + 1, num_epochs, i + 1, total_step, loss.item()))               \n",
    "        \n",
    "    \n",
    "    \n",
    "train(num_epochs, model, loaders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b788586",
   "metadata": {},
   "outputs": [],
   "source": [
    "im = train_imgs[0]\n",
    "im = im[np.newaxis,np.newaxis,:,:]\n",
    "print(im.shape)\n",
    "im = torch.from_numpy(im).float()\n",
    "print(type(im))\n",
    "t = model(im)\n",
    "plt.imshow(t[0,0,:,:].detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e9c8918",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot multiple\n",
    "figure = plt.figure(figsize=(15, 12))\n",
    "cols, rows =4,4\n",
    "for i in range(1, int((cols * rows)/2) + 1):\n",
    "    sample_idx = torch.randint(len(test_imgs), size=(1,)).item()\n",
    "    noisy, clean = test_imgs[sample_idx], test_imgs0[sample_idx]\n",
    "    noisy = noisy[np.newaxis,np.newaxis,:,:]\n",
    "    output = model(torch.from_numpy(noisy).float())\n",
    "    output = output.detach().numpy()\n",
    "    \n",
    "    figure.add_subplot(rows,cols,2*i - 1)\n",
    "    plt.title('noisy input')\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(noisy.squeeze(),cmap=\"gray\")\n",
    "    \n",
    "    figure.add_subplot(rows,cols,2*i)\n",
    "    plt.title('net denoised')\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(output.squeeze(),cmap=\"gray\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdb3a866",
   "metadata": {},
   "source": [
    "## Homework:\n",
    "\n",
    "### Run an image classification model in the CIFAR10 dataset:\n",
    "You will need to:\n",
    "- Load the data, and load into dataloaders\n",
    "- Define a network (see end)\n",
    "- Define an optimizer (recommened Stochastic Gradient Descent with lr = 0.01)\n",
    "- Define loss function (reccommend CrossEntropyLoss (note, a softmax function should not be used in the final layer of the network when using CrossEntropy))\n",
    "- Run a training loop\n",
    "- Write some code which print a montage of images, with the title on each subplot as the ground truth label and the predicted label\n",
    "\n",
    "Some remarks:\n",
    "- The images in CIFAR10 are RGB images, and are therefore of shape $(32,32,3)$, unlike MNIST which were greyscale and of shape $(28,28)$.\n",
    "- Pytorch requires inputs to a convolutional layer to be of shape (BatchSize,Channels,Height,Width). When using dataloaders this is done automatically, and so during your training loop you won't need to worry about this. However, if defining your own function (for example for plotting the output of the model), executing: \"img, label = train_data[0]\" will error as img is of shape (3,32,32), and not (1,3,32,32) -- (as you are essentially passing a batch of 1 when predicting a single image.\n",
    "- A similar problem here is when plotting the image. matplotlib expects your image to be of shape (32,32,3) (i.e. the channels to be the final dim). To transpose an image from shape (3,32,32) to (32,32,3), use the np.transpose function as in: img = img.np.transpose(img,[1,2,0])\n",
    "## Network:\n",
    "You can in principle define your own architecture, but I recommend making a simple network according to the following pseduocode\n",
    "* Conv2D (with filters 6, padding = 1) (remark: in_channels will be 3 here as RGB image!)\n",
    "* ReLU, MaxPool\n",
    "* Conv2D (with filters 16, padding = 1) \n",
    "* ReLU, MaxPool\n",
    "* Flatten\n",
    "* FullyConnectedLayer, 120\n",
    "* FullyConnectedLayer, 84\n",
    "* FullyConnectedLayer, 10\n",
    "\n",
    "As we aren't using softmax, the predicted class of the network will be the entry with the maximum value.\n",
    "\n",
    "### This network contains a relatively low number of parameters. This is so it will train in a reasonable amount of time on your machine. As a result, your network accuracy will not be amazing. Do not worry about this.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5f6d2d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "train_data = datasets.CIFAR10(root='data', \n",
    "                            train=True,\n",
    "                            transform=ToTensor())\n",
    "test_data = datasets.CIFAR10(\n",
    "    root = 'data', \n",
    "    train = False, \n",
    "    transform = ToTensor()\n",
    ")\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f542043a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "loaders = {\n",
    "    'train' : torch.utils.data.DataLoader(train_data, \n",
    "                                          batch_size=100, \n",
    "                                          shuffle=True, \n",
    "                                          num_workers=1),\n",
    "    \n",
    "    'test'  : torch.utils.data.DataLoader(test_data, \n",
    "                                          batch_size=100, \n",
    "                                          shuffle=True, \n",
    "                                          num_workers=1),\n",
    "}\n",
    "loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76441c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# Plot an image with the class\n",
    "\n",
    "i=23# change index\n",
    "im, label = train_data[i]\n",
    "print('image shape:' , im.shape) \n",
    "im = np.transpose(im, [1,2,0])\n",
    "print('image shape:' , im.shape) \n",
    "print('label is', label)\n",
    "print('corresponding class is:', classes[label])\n",
    "plt.imshow(im)\n",
    "plt.title(classes[train_data.targets[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b867d1a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define network, loss function, optimizer, and write training loop. Finally plot a montage of results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a727a959",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
